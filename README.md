# ğŸ“ˆ Clickstream Analytics Pipeline (Kafka â†’ Spark â†’ S3 â†’ Athena)

This project builds a real-time data pipeline that captures user behavior and purchase activity, processes the data, stores it in the cloud, and makes it queryable using serverless SQL.

---

## ğŸš€ What This Project Does

- Simulates **user click events** and **order placed events** using a Kafka producer
- Streams data from Kafka using **Apache Spark Structured Streaming**
- Writes partitioned **Parquet files** to **Amazon S3** (compressed with Snappy)
- Enriches order data with user click context (like location)
- Makes all data queryable using **Amazon Athena**

---

## ğŸ” Workflow Overview

```text
[ Python Producer (Faker) ]
            â¬‡
     [ Kafka Topics ]
            â¬‡
[ Spark Streaming Jobs (user + order) ]
            â¬‡
[ Partitioned Parquet on S3 ]
            â¬‡
   [ Athena External Tables ]
            â¬‡
     [ SQL-based Insights ]
```

---

## ğŸ”§ Tech Stack

- **Kafka** â€“ event streaming and ingestion
- **Spark (Structured Streaming)** â€“ real-time data processing
- **Amazon S3** â€“ cloud storage for Parquet files
- **Athena** â€“ serverless querying engine
- **Glue Catalog** â€“ (optional) for table metadata
- **Python + Faker** â€“ mock event generation

---

## âœ… Final Outcomes

- ğŸ’¾ **Raw user and order events** stored in S3, partitioned by date and hour  
- ğŸ”— **Joined data** (orders enriched with user location) for analytics  
- ğŸ” **Athena tables** created over S3 to support ad hoc SQL queries  
- ğŸ“Š **Sample business use cases** implemented via SQL, such as:
  - Average Order Value by Payment Method
  - Most Popular Items sold
  - Most Repeated purchases per customer
  - Top-spending users in a week
  - Total Order Volume by City

This setup gives you a production-like simulation of a clickstream pipeline, with zero infrastructure for querying and full compatibility with modern BI tools.

---
